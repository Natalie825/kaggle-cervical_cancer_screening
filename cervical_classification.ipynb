{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intel Cervical Cancer Screening\n",
    "### April 21, 2017\n",
    "## Satchel Grant\n",
    "\n",
    "### Overview\n",
    "This pipeline was made for the [Intel and MobileODT Cervical Cancer Screening](https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening) competition on [Kaggle](https://www.kaggle.com/). The goal is to classify cervix images into 1 of 3 types of cervix. This assists in early detection and treatment of cancer.\n",
    "\n",
    "This notebook shows several different convolutional neural network approaches.\n",
    "\n",
    "The bulk of the Python code used to make this notebook is stored in seperate .py files distributed throughout the project folder. I will provide a link to appropriate files when using a function that is defined elsewhere in the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up\n",
    "See the [README](./README.md) for creating an environment with all the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Internal Imports\n",
    "from utilities import inout\n",
    "from utilities import image_manipulation as imanip\n",
    "from utilities import miscellaneous as misc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read Paths\n",
    "\n",
    "The images are stored as jpg files in seperate folders depending on their classification. I use the function [read_paths()](./utilities/inout.py) (`utilities.inout.py` line 14) to read in the image paths as strings. The function uses the image's folder name to get the corresponding label and returns the image paths and labels as two parallel arrays.\n",
    "\n",
    "These lists will later be used in a generator to read in the images into numpy arrays. It would be easier from a coding perspective to read all the images into memory as numpy arrays, but the size of the dataset is too large for the RAM space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The training images are assumed to stored in the 'train' folder with subfolders 'Type_1', 'Type_2', 'Type_3'. The extra training images are assumed to be stored in a folder named 'extra_train' with subfolders 'Type_1', 'Type_2', 'Type_3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in file paths of images to be resized\n",
    "image_paths = []\n",
    "labels = []\n",
    "training_folders = ['data/train', 'data/extra']\n",
    "for folder in training_folders:\n",
    "    new_paths, new_labels, n_classes = inout.read_paths(folder)\n",
    "    if len(new_paths) > 0:\n",
    "        image_paths += new_paths\n",
    "        labels += new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_paths, labels = shuffle(image_paths, labels)\n",
    "histogram_dict = misc.histdict(labels, n_classes)\n",
    "\n",
    "type1_count = histogram_dict[0]\n",
    "type2_count = histogram_dict[1]\n",
    "type3_count = histogram_dict[2]\n",
    "\n",
    "print(\"Number of Type 1 Images:\", type1_count)\n",
    "print(\"Number of Type 2 Images:\", type2_count)\n",
    "print(\"Number of Type 3 Images:\", type3_count)\n",
    "print(\"Total Number of data samples: \" + str(len(image_paths)))\n",
    "print(\"Number of Classes: \" + str(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Resizing\n",
    "The images come from various sources and are of varying sizes. Additionally, some of the image data has been corrupted.\n",
    "\n",
    "Neural networks need the image size to be constant. Another consideration is that the images need to be small enough so as to leave RAM space for the model to train, but large enough that important characteristics are distinguishable for classification. I semi-arbitrarily chose 256X256 px for the image size for my model. The reason was that 200-300 X 200-300 px are common image sizes for ImageNet models. I figured that if results were terrible, it would be easy to try a different size image.\n",
    "\n",
    "I do the resizing using the [resize()](./utilities/image_manipulation.py) function (`utilities.image_manipulation.py` line 18). This function resizes the images while maintaining their aspect ratio. It then fills any black space with random color values.\n",
    "\n",
    "I first tried resizing the images without maintaining the aspect ratio. This gave poor results. Maintaining the aspect ratio gave better results.\n",
    "\n",
    "In the resize function, there is the option to save a mirrored version of the resized image. This can be used to boost the number of images for a specific cervix type. Given the relative distribution of data, I choose to add mirrored versions of images that fall into cervical types 1 and 3. This helps equalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_img_shapes = [(299,299,3),(256,256,3)]\n",
    "dest_folders = ['incept_imgs','resized_imgs']\n",
    "\n",
    "for new_img_shape,dest_folder in zip(new_img_shapes,dest_folders):\n",
    "    for i,path,label in zip(count(),image_paths,labels):\n",
    "        split_path = path.split('/')\n",
    "        new_path = 'size'+str(new_img_shape[0])+'_'+split_path[-1]\n",
    "        new_path = '/'.join([dest_folder]+split_path[:-1]+[new_path])\n",
    "        add_flip = True\n",
    "        if label == 1:\n",
    "            add_flip = False\n",
    "\n",
    "        # Used to exclude corrupt data\n",
    "        try:\n",
    "            imanip.resize(path, maxsizes=new_img_shape,\n",
    "                                save_path=new_path,\n",
    "                                add_flip=add_flip)\n",
    "        except OSError:\n",
    "            print(\"Error at path \" + path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue with Specified Dataset\n",
    "The next cell reads in the resized image paths to be used for the rest of the project. The inception net gave poor results, so we will continue with the 256x256 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resized_image_locations = ['data/resized_imgs/train','data/resized_imgs/extra']\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "for i,root_path in enumerate(resized_image_locations):\n",
    "    new_paths, new_labels, n_classes = inout.read_paths(root_path)\n",
    "    if len(new_paths) > 0:\n",
    "        image_paths += new_paths\n",
    "        labels += new_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training/Validation Split\n",
    "\n",
    "I seperate the training data into two groups. One if for actually training the model, the other is to track how well the model is doing. It is important to validate the model with data that has not been used for training so that you can tell if your model is overfitting or underfitting the data. Differences in the training and validation accuracy can indicate the degree to which the model is overfitting or underfitting. This information can be used to decide when and how much dropout or regularization to use. It can also indicate when to stop training.\n",
    "\n",
    "I portion 80% of the data for training and the other 20% for validation. After splitting the data, I save the paths and labels to csvs denoting when dataset they belong to. This insures that the two dataset do not get mixed after being established. This in turn gives flexibility to the training process. Models can be trained intermittently on multiple machines.\n",
    "\n",
    "The training files are saved to `train_csv` and the validation are saved to `valid_csv`. The file type should be a `.csv`. The actual saving is done through the [save_paths()](./utilities/inout.py) function (`utilities.inout.py` line 48).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_paths, labels = shuffle(image_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_csv = 'csvs/train_set.csv'\n",
    "valid_csv = 'csvs/valid_set.csv'\n",
    "\n",
    "training_portion = .8\n",
    "split_index = int(training_portion*len(image_paths))\n",
    "X_train_paths, y_train = image_paths[:split_index], labels[:split_index]\n",
    "X_valid_paths, y_valid = image_paths[split_index:], labels[split_index:]\n",
    "\n",
    "print(\"Train size: \")\n",
    "print(len(X_train_paths))\n",
    "print(\"Valid size: \")\n",
    "print(len(X_valid_paths))\n",
    "\n",
    "inout.save_paths(train_csv, X_train_paths, y_train)\n",
    "inout.save_paths(valid_csv, X_valid_paths, y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read In Split Sets\n",
    "\n",
    "The following cell is used if returning to this notebook after already seperating the data into training and validation sets. The paths and labels for the datasets are read in from the desired csv files using [get_split_data()](./utilities/inout.py) (`utilities.inout.py` line 76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_csv = 'csvs/train_set.csv'\n",
    "valid_csv = 'csvs/valid_set.csv'\n",
    "\n",
    "X_train_paths, y_train = inout.get_split_data(train_csv)\n",
    "X_valid_paths, y_valid = inout.get_split_data(valid_csv)\n",
    "n_classes = max(y_train)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "The following cell converts the labels into one-hot vectors using [one_hot_encode()](./utilities/image_manipulation.py) (`utilities.image_manipulation.py` line 192).\n",
    "\n",
    "One hot vectors represent the label of a data sample as a vector of zeros with a single 1. The index of the 1 value corresponds to the label. One-hot encoding is a useful representation of the truth labels because it is an easy format to produce from the neural net. This makes the loss easy to calculate and propagate backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train = imanip.one_hot_encode(y_train, n_classes)\n",
    "y_valid = imanip.one_hot_encode(y_valid, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Image Augmentations\n",
    "\n",
    "The following cells are to demonstrate the types of image augmentations that are used to incease the effective size of the training data.\n",
    "\n",
    "I leave the images in float32 format instead of uint8 to save your eyes from... intrusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "demo_img = mpimg.imread(X_train_paths[0])\n",
    "plt.imshow(demo_img.astype(np.float32))\n",
    "plt.title('Unaltered Demo Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Rotations\n",
    "\n",
    "The images are rotated using [rotate()](./utilities/image_manipulation.py) (`utilities.image_manipulation.py` line 67). The function rotates them by the specified angle and the resulting blank space is filled in with random pixel values.\n",
    "\n",
    "I've been using the somewhat unfounded hypothesis that the classifier will be unaffected by image rotations up to 45 degrees. I initially thought that even a full 180 degree flip would have little affect, but 45 degrees seems less likely to cause problems while still giving a large degree of variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rotation_angle = 45\n",
    "aug_img = imanip.rotate(demo_img,rotation_angle)\n",
    "plt.imshow(aug_img)\n",
    "plt.title('Rotated Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Translations\n",
    "\n",
    "The images are translated using the [translate()](./utilities/image_manipulation.py) function (`utilities.image_manipulation.py` line 87). The function translates each pixel in the image by the specified amount. Resulting blank space is filled with random pixel values.\n",
    "\n",
    "It is important to prevent translations from cutting off important features in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "row_displacement = 20\n",
    "col_displacement = 20\n",
    "aug_img = imanip.translate(demo_img.astype(np.float32),row_displacement,col_displacement)\n",
    "plt.imshow(aug_img)\n",
    "plt.title('Translated Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Scalings\n",
    "\n",
    "The images are scaled using the [random_zoom()](./utilities/image_manipulation.py) function (`utilities.image_manipulation.py` line 119). The function randomly scales the image by the specified amount. Resulting blank space is filled with random pixel values.\n",
    "\n",
    "It is important to prevent scalings from cutting off important features in the image and from reducing the resolution too low for important feature detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_zoom = 1/6\n",
    "aug_img = imanip.random_zoom(demo_img,max_zoom)\n",
    "plt.imshow(aug_img)\n",
    "plt.title('Scaled Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Image Generation\n",
    "\n",
    "As stated earlier in the notebook, there are too many images to read them all into memory as numpy arrays. Instead, we can create a generator to read the images into memory in batches. This also gives us the opportunity to add random augmentations to effectively increase the size of our dataset.\n",
    "\n",
    "The following cells create generators using the [image_generator()](./utilities/inout.py) function (`utilities.inout.py` line 193). An individual generator is made for both the training and validation sets. The generator for the training set adds randomly augmented images to the image batches. The random augmentations fall into one of the manipulation types demonstrated in the `Image Augmentations` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 110\n",
    "\n",
    "add_random_augmentations = True\n",
    "resize_dims = None\n",
    "\n",
    "\n",
    "n_train_samples = len(X_train_paths)\n",
    "train_steps_per_epoch = misc.get_steps(n_train_samples,batch_size,n_augs=1)\n",
    "\n",
    "n_valid_samples = len(X_valid_paths)\n",
    "valid_steps_per_epoch = misc.get_steps(n_valid_samples,batch_size,n_augs=0)\n",
    "\n",
    "train_generator = inout.image_generator(X_train_paths,y_train,batch_size,\n",
    "                                        resize_dims=resize_dims,\n",
    "                                        randomly_augment=add_random_augmentations)\n",
    "valid_generator = inout.image_generator(X_valid_paths, y_valid, batch_size, \n",
    "                                        resize_dims=resize_dims,rand_order=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training\n",
    "\n",
    "This section demonstrates multiple trainings. The models are stored in the [models](./models) folder. The weights are stored in the [weights](./weights) folder.\n",
    "\n",
    "The order of the batchnorm, maxpooling, and dropout has been an intersting experiment for this project. It is these sorts of choices, that can make model architecture more of an art than a science. The best model generalization has resulted from the order I just described. I have not developed a way to discover exactly why, but my hypothesis is as follows. \n",
    "\n",
    "Batch normalization uses the average of all activations in a layer to normalize the batch's layer of activations (hence 'batch normalization). With more activations in the normalization process, the less any one activation can dominate the normalization. Thus batchnorm should be more robust before the maxpooling.\n",
    "\n",
    "Next, it is important to use dropout after maxpooling because the dropout will have a more intense affect. This is because the affects of dropout can potentially go unnoticed if done before a maxpooling layer. If values that will be excluded in the maxpooling process are dropped prior to the maxpooling, then the dropout has no affect. Thus to ensure the dropout has an impact on the next convolutional layer, it should be done after the maxpooling.\n",
    "\n",
    "Again, I should note that I have not proved these ideas. This is just an attempt to explain empirical findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CNN Model (w/o 1x1 leading layers)\n",
    "This model is defined in the [cnn_model()](./models/model.py) function (`models.model.py` line 6) and has proven to be the best approach so far with an approximate 66% accuracy on the validation set. See the [README](./README.md) for a detailed diagram of the model layers.\n",
    "\n",
    "The basic approach is to use a 3x3 and a 5x5 filter at each convolutional layer of the model followed by batchnorm, maxpooling, and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from models import model as mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "image_shape = (256,256,3)\n",
    "\n",
    "first_conv_shapes = [(4,4),(3,3),(5,5)]\n",
    "conv_shapes = [(3,3),(5,5)]\n",
    "conv_depths = [12,12,11,8,8]\n",
    "dense_shapes = [100,50,n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs, outs = mod.cnn_model(first_conv_shapes, conv_shapes, conv_depths, dense_shapes, image_shape, n_classes)\n",
    "\n",
    "model = Model(inputs=inputs,outputs=outs)\n",
    "\n",
    "learning_rate = .0001\n",
    "for i in range(20):\n",
    "    if i > 4:\n",
    "        learning_rate = .00001 # Anneals the learning rate\n",
    "    adam_opt = optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "    history = model.fit_generator(train_generator, train_steps_per_epoch, epochs=1,\n",
    "                        validation_data=valid_generator,validation_steps=valid_steps_per_epoch, max_q_size=1)\n",
    "    model.save('./weights/cpu_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CNN Model (with 1x1 leading layers)\n",
    "\n",
    "This model is defined in the [cnn_model_1x1()](./models/model.py) function (`models.model.py` line 44) and has proven to be the second best approach so far with an approximate 60% accuracy on the validation set. See the [README](./README.md) for a detailed diagram of the model layers.\n",
    "\n",
    "The basic approach is to use a 1x1 filter prior to the 3x3 and a 5x5 filter at each convolutional layer of the model followed by batchnorm, maxpooling, and dropout. The leading 1x1 effectively allows for scaling of the activation depth. This allows the model to be built both wider and deeper without exceeding the limits of the RAM. The architecture for this model was heavily inspired by Google's inception models. \n",
    "\n",
    "I had high hopes for this style of model, and the training process is much quicker than without the 1x1 filter layers, but it has been difficult to prevent the model from overfitting. In attempt to solve this problem, I will continue to increase dropout, increase the training set size, implement pseudo labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from models import model as mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "batch_size = 100\n",
    "image_shape = (256,256,3)\n",
    "\n",
    "ones_depth=25\n",
    "conv_shapes=[(3,3),(5,5)]\n",
    "conv_depths=[10,12,14,16,20]\n",
    "dense_shapes=[200,70,3]\n",
    "image_shape=(256,256,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs, outs = mod.cnn_model_1x1(conv_shapes, conv_depths, dense_shapes, image_shape, n_classes,ones_depth)\n",
    "\n",
    "model = Model(inputs=inputs,outputs=outs)\n",
    "\n",
    "adam_opt = optimizers.Adam(lr=.0001)\n",
    "for i in range(20):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "    history = model.fit_generator(train_generator, train_steps_per_epoch, epochs=1,\n",
    "                        validation_data=valid_generator,validation_steps=valid_steps_per_epoch, max_q_size=1)\n",
    "    model.save('./weights/model_1x1.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Transfer Model\n",
    "\n",
    "_Note: The transfer learning cells will not work unless you have resized the images to 299x299x3_\n",
    "\n",
    "Transfer learning is the process of repurposing a previously trained model. These models are made by smart people with lots of resources. It is easy to use pieces of their trained model as the backbone of a new model. \n",
    "\n",
    "Feature extraction and fine tuning are the two approaches in transfer learning. \n",
    "\n",
    "Feature extraction is the process of removing the output layer of the transfer model and training a new, small model (usually only dense layers) on the outputs. Thus no training occurs on the transferred model. \n",
    "\n",
    "Finetuning is the process of removing the output layer and appending layers to the model and then training the entire model.\n",
    "\n",
    "I attempted both feature extraction and fine-tuning on Google's Inception V4 model. Both gave poor results. See the `Failed Experiments` section in the [README](./README.md) for a more detailed analysis.\n",
    "\n",
    "This model is defined throughout the [inceptionV4.py](./models/model.py) file. The bulk of the code was taken from an outside source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from models import inceptionV4 as incept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_extraction_only = False\n",
    "\n",
    "init, flat_layer, weights = incept.create_inception_v4()\n",
    "flat_layer = Dense(1001, activation='elu')(flat_layer)\n",
    "outs = Dense(3, activation='elu')(flat_layer)\n",
    "\n",
    "model = Model(inputs=init,outputs=outs)\n",
    "model.load_weights(weights, by_name=True)\n",
    "\n",
    "if feature_extraction_only:\n",
    "    for i in range(len(model.layers[:-3])):\n",
    "        model.layers[i].trainable = False\n",
    "\n",
    "learning_rate = .0001\n",
    "for i in range(20):\n",
    "    if i > 4:\n",
    "        learning_rate = .00001 # Anneals the learning rate\n",
    "    adam_opt = optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "    history = model.fit_generator(train_generator, train_steps_per_epoch, epochs=1,\n",
    "                        validation_data=valid_generator,validation_steps=valid_steps_per_epoch, max_q_size=1)\n",
    "    model.save('./weights/inception_model.h5')\n",
    "print('History test', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predictions\n",
    "The following cells make predictions on the test set.\n",
    "\n",
    "Similar to the training images, I first read in the image paths. I then create a seperate process for reading in and resizing the test images using ThreadPool. This allows the images to be processed while the model evaluates the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = './data/test'\n",
    "model_path = 'weights/model_1x1.h5'\n",
    "model_type = 2\n",
    "\n",
    "resize_dims = (256,256,3)\n",
    "test_divisions = 20 # Used for segmenting image evaluation in threading\n",
    "batch_size = 100 # Batch size used for keras predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if model_type == 1:\n",
    "    ins, outs = mod.cnn_model()\n",
    "elif model_type == 2:\n",
    "    ins, outs = mod.cnn_model_1x1()\n",
    "model = Model(inputs=ins,outputs=outs)\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_paths, test_labels, _ = inout.read_paths(data_path,no_labels=True)\n",
    "print(str(len(test_paths))+' testing images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seperate process uses the function [convert_images()](./utilities/inout.py) (`utilities.inout.py` line 90). This function simply reads in the images from the given paths and returns the images with their corresponding labels. `convert_images()` can also resize the images when reading them in. In this case, we resize the test images to the required shape. It defaults to maintaining the aspect ratio, however, this is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(processes=1)\n",
    "portion = len(test_paths)//test_divisions+1 # Number of images to read in per pool\n",
    "\n",
    "async_result = pool.apply_async(inout.convert_images,(test_paths[0*portion:portion*(0+1)],\n",
    "                                                test_labels[0*portion:portion*(0+1)],resize_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_base_time = time.time()\n",
    "test_imgs = []\n",
    "predictions = []\n",
    "for i in range(1,test_divisions+1):\n",
    "    base_time = time.time()\n",
    "\n",
    "    print(\"Begin set \" + str(i))\n",
    "    while len(test_imgs) == 0:\n",
    "        test_imgs,_ = async_result.get()\n",
    "    img_holder = test_imgs\n",
    "    test_imgs = []\n",
    "\n",
    "    if i < test_divisions:\n",
    "        async_result = pool.apply_async(inout.convert_images,(test_paths[i*portion:portion*(i+1)],\n",
    "                                                        test_labels[0*portion:portion*(0+1)],resize_dims))\n",
    "\n",
    "    predictions.append(model.predict(img_holder,batch_size=batch_size,verbose=0))\n",
    "    print(\"Execution Time: \" + str((time.time()-base_time)/60)+'min\\n')\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "print(\"Total Execution Time: \" + str((time.time()-total_base_time)/60)+'mins')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Predictions\n",
    "The loss format for kaggle is the negative sum of the logs of the prediction confidences for the truth label.\n",
    "\n",
    "loss = sum(log(correct class confidence))/num_samples for each correct class. \n",
    "\n",
    "This means that a high confidence on a correct prediction is minimally penalized whereas a low confidence on the actual label is maximally penalized.\n",
    "\n",
    "The following cell sets a specified confidence for the predicted class using the [confidence()](./utilities/miscellaneous.py) function (`utilities.miscellaneous.py` line 1). The remaining confidence (out of 1) is equally distributed to the other 9 classes.\n",
    "\n",
    "The predictions are then saved to the specified save file using the [save_predictions()](./utilities/inout.py) function (`utilities.inout.py` line 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conf = .95 # Prediction confidence\n",
    "save_file = 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = misc.confidence(predictions, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "header = 'image_name,Type_1,Type_2,Type_3'\n",
    "inout.save_predictions(save_file, test_labels, predictions, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Results\n",
    "\n",
    "The following cell discusses and analyzes the results from the test set predictions.\n",
    "\n",
    "Kaggle evaluate submissions based on the calculated log-loss instead of the accuracy. Using Kaggle's log-loss function and the public evaluation, we can extrapolate our accuracy. \n",
    "\n",
    "This is how the loss is calculated on Kaggle (please forgive the formatting...).\n",
    "\n",
    "loss = 1/num_total_images x (num_correct_predictions x log(confidence) + \n",
    "                            num_wrong_predictions x log((1-confidence)/(n_classes-1)))\n",
    "\n",
    "loss x n = num_right x log(confidence) + num_wrong x log((1-confidence)/(n_classes-1))\n",
    "\n",
    "The following cell tries different accuracies to approximate the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utilities.inout as inout\n",
    "## Optionally read in test submission from csv\n",
    "test_paths, predictions = inout.read_submission('csvs/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 505.43616\n",
      "Calculated loss: 505.6378849578217\n",
      "Accuracy: ~66.01562999157008%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "kaggle_loss = 0.98718\n",
    "data_portion = 1.0\n",
    "num_samples = len(test_paths)*data_portion\n",
    "confidence = max(predictions[0])\n",
    "n_classes = len(predictions[0])\n",
    "\n",
    "kaggle_loss = kaggle_loss*num_samples\n",
    "accuracy = 0.5\n",
    "calculated_loss = 0\n",
    "while (calculated_loss-kaggle_loss)**2 > 1 and accuracy <= 1:\n",
    "    accuracy += .0000001\n",
    "    num_right = int(num_samples*accuracy)\n",
    "    num_wrong = num_samples-num_right\n",
    "    calculated_loss = -(num_right*math.log(confidence) + num_wrong*math.log((1-confidence)/(n_classes-1)))\n",
    "print(\"Actual loss: \" + str(kaggle_loss))\n",
    "print(\"Calculated loss: \" + str(calculated_loss))\n",
    "print(\"Accuracy: ~\" + str(accuracy*100) + '%')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
